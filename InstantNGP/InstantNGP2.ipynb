{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60a8d77f",
   "metadata": {},
   "source": [
    "# InstantNGP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587118a5",
   "metadata": {},
   "source": [
    "- NeRF 분야에 적용할 수 있는 InstantNGP 모델을 만듭니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d19deaf",
   "metadata": {},
   "source": [
    "## Setting tiny-cuda-nn framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27cf53fd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu117\n",
      "Requirement already satisfied: torch in c:\\users\\user\\anaconda3\\lib\\site-packages (2.0.0+cu117)\n",
      "Requirement already satisfied: torchvision in c:\\users\\user\\anaconda3\\lib\\site-packages (0.15.1+cu117)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\user\\anaconda3\\lib\\site-packages (2.0.1+cu117)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (2.8.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (3.6.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (2.11.3)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (4.3.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (1.10.1)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\anaconda3\\lib\\site-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from torchvision) (9.2.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\anaconda3\\lib\\site-packages (from torchvision) (1.21.5)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->torchvision) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->torchvision) (2022.9.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->torchvision) (3.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\user\\anaconda3\\lib\\site-packages (from sympy->torch) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "# First, check the CUDA version on the PyTorch site. \n",
    "# Second, download the CUDA toolkit for your version. \n",
    "# Third, execute the command suggested on the PyTorch site. \n",
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b316b9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import PyTorch library and check CUDA is available. \n",
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad92b6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "# Check your OS,and make sure C++ is installed and compiler path is defined as a system environment variable . \n",
    "# Download the tiny-cuda-nn-framework.\n",
    "!pip install git+https://github.com/NVlabs/tiny-cuda-nn/#subdirectory=bindings/torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f25d81a",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "397da644",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json \n",
    "import numpy as np \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae1b7ff",
   "metadata": {},
   "source": [
    "## Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fb8ba09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 3, 1920, 1080])\n",
      "torch.Size([50, 1920, 1080, 3])\n"
     ]
    }
   ],
   "source": [
    "# We will use smaller fox dataset from InstantNGP github repository.\n",
    "# So, download the data from the InstantNGP github repository and place it in the virtual environment.\n",
    "# Please refer to the following address 'https://github.com/NVlabs/instant-ngp/tree/master/data/nerf/fox'.\n",
    "# Save that data to the 'data' folder in the default path of the virtual environment.\n",
    "\n",
    "# Specify the path where the data is stored.\n",
    "image_path = './data/images'\n",
    "\n",
    "# Define the images as Image objects and append them in the list. \n",
    "images = [Image.open(os.path.join(image_path,image)) for image in os.listdir(image_path)]\n",
    "\n",
    "# Define transformations to apply.\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Apply the transformations to the list of images.\n",
    "img_tensor_list = []\n",
    "for image in images:\n",
    "    img_tensor_list.append(transform(image))\n",
    "\n",
    "stacked_img = torch.stack(img_tensor_list, dim = 0)\n",
    "\n",
    "print(stacked_img.shape)\n",
    "\n",
    "stacked_img = stacked_img.permute(0, 2, 3, 1).contiguous()\n",
    "print(stacked_img.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "44d8e734",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torchvision.transforms' has no attribute 'camera_angle_x'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14928\\1077040155.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mtransform_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtransform_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform_params\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtransforms_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mtransform\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mtransform_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mtransform_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mtransform_sequence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrasforms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCompose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransform_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torchvision.transforms' has no attribute 'camera_angle_x'"
     ]
    }
   ],
   "source": [
    "# We will use smaller fox dataset from InstantNGP github repository.\n",
    "# So, download the data from the InstantNGP github repository and place it in the virtual environment.\n",
    "# Please refer to the following address 'https://github.com/NVlabs/instant-ngp/tree/master/data/nerf/fox'.\n",
    "# Save that data to the 'data' folder in the default path of the virtual environment.\n",
    "with open('./data/transforms.json', 'r') as f:\n",
    "    transforms_dict = json.load(f)\n",
    "\n",
    "transform_list = []\n",
    "for transform_name, transform_params in transforms_dict.items():\n",
    "    transform = getattr(transforms, transform_name)(**transform_params)\n",
    "    transform_list.append(transform)\n",
    "transform_sequence = trasforms.Compose(transform_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096aeb1a",
   "metadata": {},
   "source": [
    "## Building Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95e5721",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstantNeRF(Dataset):\n",
    "    def __init__(self, dataset, train = True):\n",
    "        self.dataset = dataset\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcd871d",
   "metadata": {},
   "source": [
    "## Defining Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48449da3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0607b80",
   "metadata": {},
   "source": [
    "## Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc1704d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstantNeRF(nn.module):\n",
    "    def __init__(self, ):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, );\n",
    "        pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
