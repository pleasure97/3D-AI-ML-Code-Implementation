{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# DiffusionGS - Baking Gaussian Splatting into Diffusion Denoiser for Fast and Scalable Single-stage Image-to-3D Generation"
      ],
      "metadata": {
        "id": "56DRLegJNp_X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Official GitHub - https://caiyuanhao1998.github.io/project/DiffusionGS/"
      ],
      "metadata": {
        "id": "EuQv4CNGj-wx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div align=\"center\">\n",
        "  <img src=\"https://raw.githubusercontent.com/pleasure97/3D-AI-ML-Code-Implementation/main/2025/DiffusionGS/assets/pipeline.JPG\" alt=\"Pipeline of DiffusionGS\">\n",
        "</div>"
      ],
      "metadata": {
        "id": "3c390lG2jlTy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Dataset\n",
        "\n",
        "In page 6 of the paper,\n",
        "\n",
        "> We use **Objaverse and MVImgNet** as the training sets for objects.\n",
        "\n",
        "> We center and scale each 3D object of Objaverse into $[-1, 1]^3$, and render 32 images at random viewpoints with 50 FOV.\n",
        "\n",
        "> For MVImgNet, we crop the object, remove the background, normalize the cameras, and center and scale the object to $[-1, 1]^3$."
      ],
      "metadata": {
        "id": "VDMu_82KQlxZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Objaverse Dataset"
      ],
      "metadata": {
        "id": "nsdFiX7rqH_P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1.1 Prepare Objaverse Environmnet"
      ],
      "metadata": {
        "id": "zxYpWVS07tTd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For more details of loading Objaverse dataset, you can look here - https://colab.research.google.com/drive/1ZLA4QufsiI_RuNlamKqV7D7mn40FbWoY"
      ],
      "metadata": {
        "id": "lkV-HjT1u_J8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install objaverse --upgrade --quiet\n",
        "\n",
        "import objaverse\n",
        "objaverse.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "fRajVD8eoO_f",
        "outputId": "c87d5721-a124-4bc4-9c90-72202a2ac7b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0.1.7'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each object has a unique corresponding ID (universal identifier)."
      ],
      "metadata": {
        "id": "5NMFtXzCpIGc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "uids = objaverse.load_uids()\n",
        "print(f\"length of uids : {len(uids)}\")\n",
        "print(f\"type of uids : {type(uids)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWOH7LVwqoKJ",
        "outputId": "c089241e-4631-4c8a-a6d7-89103137db6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of uids : 798759\n",
            "type of uids : <class 'list'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "uids[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MbjcrKdqueV",
        "outputId": "9ca535a7-fd63-4fe3-c801-a607e99b2a6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['8476c4170df24cf5bbe6967222d1a42d',\n",
              " '8ff7f1f2465347cd8b80c9b206c2781e',\n",
              " 'c786b97d08b94d02a1fa3b87d2e86cf1',\n",
              " '139331da744542009f146018fd0e05f4',\n",
              " 'be2c02614d774f9da672dfdc44015219',\n",
              " 'efd35e7d21ac482688c294e3b6c9f74e',\n",
              " '21d5f90dbc9f4f229b0faa7b56b67f3e',\n",
              " 'dcd33159a0864de388de3a08f55e604a',\n",
              " 'a7ad32b5d4d84ee5a40ebbd86da4dbe4',\n",
              " '7d6a14874eed48c2b720f0d1adfe6dd9']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can get the object annotations for each of object using `objaverse.load_annotations()`."
      ],
      "metadata": {
        "id": "0CN7ZGlqsrRe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "annotations = objaverse.load_annotations(uids[:1])"
      ],
      "metadata": {
        "id": "DPaTMDCIrDDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're going to use multiprocessing to download the objects."
      ],
      "metadata": {
        "id": "3iST6yrnsSw3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import multiprocessing\n",
        "processes = multiprocessing.cpu_count()\n",
        "processes"
      ],
      "metadata": {
        "id": "Z5HmbnOItPV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`objaverse.load_objects()` takes in a list of object UIDs and optionally the number of download processes, and returns a map from each object UIDs to its `.glb` file location on disk."
      ],
      "metadata": {
        "id": "zMM-Oq-stbhi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "objaverse_objects = objaverse.load_objects(uids=uids[:10], download_processes=processes)\n",
        "objaverse_objects"
      ],
      "metadata": {
        "id": "UJIUXFlyu9et"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's load up one of the `.glb` files to visualize it."
      ],
      "metadata": {
        "id": "c9AtNVaKwKrA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install trimesh --quiet"
      ],
      "metadata": {
        "id": "TDLbQj18wZ-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import trimesh\n",
        "trimesh.load(list(objaverse_objects.values())[1]).show()"
      ],
      "metadata": {
        "id": "bafb_icswbje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1.2 Center and Scale Each Objaverse Object"
      ],
      "metadata": {
        "id": "yb_ou-Q-73qO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pyglet==v1.5.28"
      ],
      "metadata": {
        "id": "zO66XoCajYTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/utils.py\n",
        "import trimesh\n",
        "import numpy as np\n",
        "def center_scale_mesh(mesh: trimesh.Trimesh):\n",
        "\n",
        "  # Calculate the minimum, maximum bound and center\n",
        "  min_bound = mesh.vertices.min(axis=0)\n",
        "  max_bound = mesh.vertices.max(axis=0)\n",
        "  center = (min_bound + max_bound) / 2\n",
        "  print(\"Mininum bound of Original: \", min_bound)\n",
        "  print(\"Maximum bound of Original: \", max_bound)\n",
        "  print(\"Center: \", center)\n",
        "\n",
        "  # Move all the vertices to center\n",
        "  mesh.vertices -= center\n",
        "\n",
        "  bound_size = max_bound - min_bound\n",
        "  max_extent = np.max(bound_size)\n",
        "\n",
        "  mesh.vertices /= (max_extent / 2)\n",
        "\n",
        "  return mesh"
      ],
      "metadata": {
        "id": "kVJ5imVg0sbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/preprocess_objaverse.py\n",
        "from utils import center_scale_mesh\n",
        "import trimesh\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def sample_random_views(num_images=32, radius=1.):\n",
        "  camera_positions = []\n",
        "  view_directions = []\n",
        "\n",
        "  for _ in range(num_images):\n",
        "      theta = np.random.uniform(-2, 2)\n",
        "      phi = np.random.uniform(-1, 1.5)\n",
        "\n",
        "      cam_x = radius * np.sin(phi) * np.cos(theta)\n",
        "      cam_y = radius * np.sin(phi) * np.sin(theta)\n",
        "      cam_z = radius * np.cos(phi)\n",
        "\n",
        "      position = np.array([cam_x, cam_y, cam_z])\n",
        "      direction = -position / np.linalg.norm(position)\n",
        "\n",
        "      camera_positions.append(position)\n",
        "      view_directions.append(direction)\n",
        "\n",
        "  return camera_positions, view_directions\n",
        "\n",
        "\n",
        "def render_images_trimesh(mesh_file, output_dir, num_images=32, fov=50, image_size=(256, 256)):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    mesh = trimesh.load(mesh_file, force='mesh')\n",
        "    mesh = normalize_mesh(mesh)\n",
        "\n",
        "    scene = trimesh.Scene()\n",
        "    scene.add_geometry(mesh)\n",
        "\n",
        "    camera_positions, view_directions = sample_random_views(num_images)\n",
        "\n",
        "    for i, (position, direction) in enumerate(zip(camera_positions, view_directions)):\n",
        "        try:\n",
        "            camera = trimesh.scene.Camera(fov=(fov, fov), resolution=image_size)\n",
        "            scene.camera = camera\n",
        "\n",
        "            look_at_matrix = trimesh.scene.cameras.look_at(points=[position], fov=fov)\n",
        "\n",
        "            scene.camera_transform = look_at_matrix\n",
        "\n",
        "            image_data = scene.save_image(resolution=image_size)\n",
        "            if image_data is not None:\n",
        "                image_path = os.path.join(output_dir, f\"{i:03d}.png\")\n",
        "                with open(image_path, 'wb') as f:\n",
        "                    f.write(image_data)\n",
        "        except ZeroDivisionError as e:\n",
        "            print(f\"[ERROR] ZeroDivisionError for view {i} : {e}\")\n",
        "            continue\n",
        "\n",
        "    print(f\"Saved {num_images} images to {output_dir}\")\n",
        "    print(f\"Saved {num_images} images to {output_dir}\")"
      ],
      "metadata": {
        "id": "AAfZB44Cwol5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, objaverse_object in enumerate(objaverse_objects.values()):\n",
        "  render_images_trimesh(objaverse_object, f\"/content/objaverse/images/{idx}\")"
      ],
      "metadata": {
        "id": "5jOU2JGzCNvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1.3 Render Objaverse Images at Random Viewpoints"
      ],
      "metadata": {
        "id": "qKC5g1VI8AU2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** It is difficult to render images in Colab, so we will sample images right away."
      ],
      "metadata": {
        "id": "t3ncUwwAOF96"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PLY_FOLDER = \"/content/objaverse/normalized\"\n",
        "IMAGES_FOLDER = \"/content/objaverse/images\"\n",
        "\n",
        "ply_files = [file for file in os.listdir(PLY_FOLDER) if file.endswith(\".ply\")]\n",
        "\n",
        "for ply_file in ply_files:\n",
        "  ply_path = os.path.join(PLY_FOLDER, ply_file)\n",
        "  image_dir = os.path.join(IMAGES_FOLDER, ply_file.split('.')[0])\n",
        "\n",
        "  sample_images_from_random_views(ply_path, image_dir)"
      ],
      "metadata": {
        "id": "2Gz9aAG_APDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 MVImgNet Dataset\n",
        "\n",
        "**Note: You should enter the required information at the following link to download MVImgNet.**- https://docs.google.com/forms/d/e/1FAIpQLSfU9BkV1hY3r75n5rc37IvlzaK2VFYbdsvohqPGAjb2YWIbUg/viewform?usp=sf_link\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-8yB10Fwwwek"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Below is the folder structure of the MVImgNet dataset.\n",
        "\n",
        "<pre>\n",
        "mvi_40/\n",
        "    ├── 0/\n",
        "    │   ├── 3a004e55/\n",
        "    │   │   ├── images/\n",
        "    │   │   │   ├── 001.jpg\n",
        "    │   │   │   ├── 002.jpg\n",
        "    │   │   │   ├── ...\n",
        "    │   │   │   ├── 031.jpg\n",
        "    │   │   ├── sparse/\n",
        "    │   ├── 3a016095/\n",
        "    │   ├── ...\n",
        "    │   ├── 38016701/\n",
        "    ├── 1/\n",
        "    ├── ...\n",
        "    ├── 266/\n",
        "</pre>\n"
      ],
      "metadata": {
        "id": "eemad5kUxApl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2.1 Crop the MVImgNet Object\n",
        "\n",
        "Since the image resolution of MVImgNet is ```(1920, 1080)```, we'll crop the images accordingly."
      ],
      "metadata": {
        "id": "5lFQyDJYwMHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MVIMGNET_DIR = \"/content/mvi_40\""
      ],
      "metadata": {
        "id": "rz8YEzN8wlxf"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/preprocess_mvimgnet.py\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "\n",
        "def crop_image(image_dir: str, crop_size: tuple=(800, 400)):\n",
        "\n",
        "  transform = transforms.Compose([transforms.CenterCrop(crop_size)])\n",
        "\n",
        "  for dirpath, dirnames, filenames in os.walk(image_dir):\n",
        "        for filename in filenames:\n",
        "            if filename.endswith(\".jpg\"):\n",
        "                img_path = os.path.join(dirpath, filename)\n",
        "\n",
        "                img = Image.open(img_path)\n",
        "\n",
        "                cropped_img = transform(img)\n",
        "\n",
        "                output_path = os.path.join(dirpath, f\"cropped_{filename}\")\n",
        "                print(f\"output path is  : {output_path}\")\n",
        "                cropped_img.save(output_path)\n",
        ""
      ],
      "metadata": {
        "id": "psuJfajqzGVA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec0015fc-fedf-4c00-fc02-03e812986e39"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/preprocess_mvimgnet.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from preprocess_mvimgnet import crop_image\n",
        "\n",
        "crop_image(MVIMGNET_DIR, crop_size=(800, 400))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbQmPcys8W46",
        "outputId": "fbca0bef-175a-4d95-bc07-98cbc8fc6177"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output path is  : /content/mvi_40/cropped_016.jpg\n",
            "output path is  : /content/mvi_40/cropped_003.jpg\n",
            "output path is  : /content/mvi_40/cropped_018.jpg\n",
            "output path is  : /content/mvi_40/cropped_027.jpg\n",
            "output path is  : /content/mvi_40/cropped_009.jpg\n",
            "output path is  : /content/mvi_40/cropped_001.jpg\n",
            "output path is  : /content/mvi_40/cropped_025.jpg\n",
            "output path is  : /content/mvi_40/cropped_013.jpg\n",
            "output path is  : /content/mvi_40/cropped_011.jpg\n",
            "output path is  : /content/mvi_40/cropped_021.jpg\n",
            "output path is  : /content/mvi_40/cropped_026.jpg\n",
            "output path is  : /content/mvi_40/cropped_007.jpg\n",
            "output path is  : /content/mvi_40/cropped_002.jpg\n",
            "output path is  : /content/mvi_40/cropped_008.jpg\n",
            "output path is  : /content/mvi_40/cropped_010.jpg\n",
            "output path is  : /content/mvi_40/cropped_031.jpg\n",
            "output path is  : /content/mvi_40/cropped_017.jpg\n",
            "output path is  : /content/mvi_40/cropped_006.jpg\n",
            "output path is  : /content/mvi_40/cropped_023.jpg\n",
            "output path is  : /content/mvi_40/cropped_015.jpg\n",
            "output path is  : /content/mvi_40/cropped_024.jpg\n",
            "output path is  : /content/mvi_40/cropped_019.jpg\n",
            "output path is  : /content/mvi_40/cropped_012.jpg\n",
            "output path is  : /content/mvi_40/cropped_028.jpg\n",
            "output path is  : /content/mvi_40/cropped_029.jpg\n",
            "output path is  : /content/mvi_40/cropped_022.jpg\n",
            "output path is  : /content/mvi_40/cropped_004.jpg\n",
            "output path is  : /content/mvi_40/cropped_005.jpg\n",
            "output path is  : /content/mvi_40/cropped_030.jpg\n",
            "output path is  : /content/mvi_40/cropped_020.jpg\n",
            "output path is  : /content/mvi_40/cropped_014.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below are examples of images before and after cropping.\n",
        "\n",
        "<div align=\"center\">\n",
        "    <img src=\"https://raw.githubusercontent.com/pleasure97/3D-AI-ML-Code-Implementation/main/2025/DiffusionGS/assets/Before Cropping.jpg\" alt=\"Before Cropping\" width=\"400\" height=\"400\" style=\"margin-right: 100px;\">\n",
        "    <img src=\"https://raw.githubusercontent.com/pleasure97/3D-AI-ML-Code-Implementation/main/2025/DiffusionGS/assets/After Cropping.jpg\" alt=\"After Cropping\" width=\"400\" height=\"400\">\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "W3IyIUxXxC3_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2.2 Remove the Background"
      ],
      "metadata": {
        "id": "wGbaWc2t8etz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rembg --quiet"
      ],
      "metadata": {
        "id": "q74NYcsE-REk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a716be0-56a0-4e56-f3f7-0e798aa13b8a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/40.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.4/40.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/54.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnxruntime --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gr--dt-n27Vj",
        "outputId": "5dee7e2f-da70-4e57-c720-567861e75872"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile -a /content/preprocess_mvimgnet.py\n",
        "from rembg import remove\n",
        "from PIL import Image\n",
        "import io\n",
        "\n",
        "def remove_background(image_dir: str, save_as_png: bool=False):\n",
        "    for dirpath, _, filenames in os.walk(image_dir):\n",
        "        for filename in filenames:\n",
        "            if filename.startswith(\"cropped_\") and filename.endswith(\".jpg\"):\n",
        "                img_path = os.path.join(dirpath, filename)\n",
        "\n",
        "                with open(img_path, 'rb') as input_file:\n",
        "                    input_data = input_file.read()\n",
        "                    output_data = remove(input_data,\n",
        "                                         alpha_matting=True,\n",
        "                                         alpha_matting_foreground_threshold=240,\n",
        "                                         alpha_matting_background_threshold=10,\n",
        "                                         alpha_matting_erode_size=5)\n",
        "\n",
        "                output_image = Image.open(io.BytesIO(output_data))\n",
        "\n",
        "                if save_as_png:\n",
        "                    output_filename = filename.replace(\".jpg\", \"_no_bg.png\")\n",
        "                    output_path = os.path.join(dirpath, output_filename)\n",
        "                    output_image.save(output_path, \"PNG\")\n",
        "                else:\n",
        "                    output_filename = filename.replace(\".jpg\", \"_no_bg.jpg\")\n",
        "                    output_path = os.path.join(dirpath, output_filename)\n",
        "\n",
        "                    rgb_image = Image.new(\"RGB\", output_image.size, (255, 255, 255))\n",
        "                    rgb_image.paste(output_image, mask=output_image.split()[3])\n",
        "                    rgb_image.save(output_path, \"JPEG\", quality=95)\n",
        "                print(f\"Saved background-removed image: {output_path}\")"
      ],
      "metadata": {
        "id": "1jXAK6nz8Dfc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "730c113f-a36d-44d4-9bdd-b5c9d12d616b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Appending to /content/preprocess_mvimgnet.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2.3 Normalize the Camera"
      ],
      "metadata": {
        "id": "ixWh9gN88jUL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aOMEA2pe-RXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2.4 Center and Scale MVImgNet Object"
      ],
      "metadata": {
        "id": "ungDVX6q8lPL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MAgIwgis-Rus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Patchify Images and Camera Condition"
      ],
      "metadata": {
        "id": "O8w5r_kr9eUV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Training"
      ],
      "metadata": {
        "id": "pMV_h5QfCaiN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "mJ92BGGb-h1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Scene-Object Mixed Training Strategy\n",
        "\n",
        "In page 3 of the paper,\n",
        "> For each scene or object, we pick up a view as the condition, $N$ views as the noisy views to be denoised, and $M$ novel views for supervision."
      ],
      "metadata": {
        "id": "e1SFPNu3HG5s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### 2.1.1 Viewpoint Selecting\n",
        "\n",
        "$$\\theta_{c d}^{(i)} \\leq \\theta_1, \\quad \\theta_{d n}^{(i, j)} \\leq \\theta_2,$$\n",
        "\n",
        "* The first constraint of the angle between viewpoint and positions\n",
        "* $\\theta_{c d}^{(i)}$ - the angle between the $i$-th noisy view position and the condition view position\n",
        "* $\\theta_{d n}^{(i, j)}$ - the angle between the $i$-th noisy view position and the $j$-th novel view position\n",
        "* $\\theta_{1}, \\theta_{2}$ - hyperparameters\n",
        "* $1 \\leq i \\leq N$\n",
        "* $1 \\leq$ $j \\leq M$\n",
        "\n",
        "In page 5 of the paper,\n",
        "> The position vector can be read from the translation of the camera-to-world (c2w) matrix of the viewpoint.\n"
      ],
      "metadata": {
        "id": "MU1colHDgtmD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mBkeC3Q52dyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "$$\n",
        "\\frac{\\vec{z}_{c o n} \\cdot \\vec{z}_{n o i s e}^{(i)}}{\\left|\\vec{z}_{\\text {con }}\\right| \\cdot\\left|\\vec{z}_{\\text {noise }}^{(i)}\\right|} \\geq \\cos \\left(\\varphi_1\\right), \\frac{\\vec{z}_{\\text {con }} \\cdot \\vec{z}_{n v}^{(j)}}{\\left|\\vec{z}_{\\text {con }}\\right| \\cdot\\left|\\vec{z}_{n v}^{(j)}\\right|} \\geq \\cos \\left(\\varphi_2\\right)\n",
        "$$\n",
        "\n",
        "* The second constraint of the angle between viewpoint orientations\n",
        "* $\\vec{z}_{c o n}$ - the forward direction vectors of the condition view\n",
        "* $\\vec{z}_{n o i s e}^{(i)}$ - the forward direction vectors of the $i$-th noisy view\n",
        "* $\\vec{z}_{n v}^{(j)}$ - the forward direction vectors of the $j$-th novel view\n",
        "* $\\varphi_1$, $\\varphi_2$ - hyperparameters"
      ],
      "metadata": {
        "id": "McGqwrYS0jWb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xcC2n2Uj2bur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### 2.1.2 Reference-Point Plücker Coordinate (RPPC)\n",
        "\n",
        "$$r=(o-(o \\cdot d) d, d)$$\n",
        "\n",
        "* $r$ - the pixel-aligned ray embeddings\n",
        "* $o$ - the position of the ray landing on the pixel\n",
        "* $d$ - the direction of the ray landing on the pixel\n",
        "\n",
        "---\n",
        "### Difference Between Original Plücker Coordinate and Reference-Point Plücker Coordinate\n",
        "\n",
        "<div align=\"center\">\n",
        "  <img src=\"https://raw.githubusercontent.com/pleasure97/3D-AI-ML-Code-Implementation/main/2025/DiffusionGS/assets/Plucker Coordinates.JPG\" alt=\"Plücker Coordinates\">\n",
        "</div>\n",
        "\n",
        "In page 5 of the paper,\n",
        "\n",
        "> Specifically, $o \\times d$ represents the rotational effect of $o$ relative to $d$, showing limitations in perceiving the relative depth and geometry.\n",
        "\n",
        "In page 6 of the paper,\n",
        "\n",
        "> Our RPPC (Reference-Point Plücker Coordinate) satisifies the translation invariance assumption of the 4D light field.\n",
        "\n",
        "> Plus, ..., our reference point can provide more information about the ray position and the relative depth, which are beneficial for the diffusion model to capture the 3D geometry of scens and objects.  "
      ],
      "metadata": {
        "id": "MXORgZM_QEiV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/RPPC.py\n",
        "# Source Code : https://github.com/echen01/ray-conditioning/blob/8e1d5ae76d4747c771d770d1f042af77af4b9b5d/training/plucker.py\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "\n",
        "def get_rays(H, W, intrinsics, c2w, jitter=False):\n",
        "  \"\"\"\n",
        "  H : image height\n",
        "  W : image width\n",
        "  intrinsics : 4 by 4 intrinsic matrix\n",
        "  c2w : 4 by 4 camera to world extrinsic matrix\n",
        "  \"\"\"\n",
        "  u, v = torch.meshgrid(torch.arange(W, device=c2w.device), torch.arange(H, device=c2w.device), indexing=\"ij\")\n",
        "  B = c2w.shape[0]\n",
        "  u, v = u.reshape(-1), v.reshape(-1)\n",
        "  u_noise = v_noise = 0.5\n",
        "  if jitter:\n",
        "    u_noise = torch.rand(u.shape, device=c2w.device)\n",
        "    v_noise = torch.rand(v.shape, device=c2w.device)\n",
        "  u, v = u + u_noise, v + v_noise # add half pixel\n",
        "  pixels = torch.stack((u, v, torch.ones_like(u)), dim=0) # (3, H * W)\n",
        "  pixels = pixels.unsqueeze(0).repeat(B, 1, 1) # (B, 3 , H * W)\n",
        "  if intrinsics.sum() == 0:\n",
        "    inv_intrinsics = torch.eye(3, device=c2w.device).tile(B, 1, 1)\n",
        "  else:\n",
        "    inv_intrinsics = torch.linalg.inv(intrinsics)\n",
        "  rays_d = inv_intrinsics @ pixels # (B, 3, H * W)\n",
        "  rays_d = c2w[:, :3, :3] @ rays_d\n",
        "  rays_d = rays_d.transpose(-1, -2) # (B, H * W, 3)\n",
        "  rays_d = F.normalize(rays_d, dim=-1)\n",
        "\n",
        "  rays_o = c2w[:, :3, 3].reshape((-1, 3)) # (B, 3)\n",
        "  rays_o = rays_o.unsqueeze(1).repeat(1, H * W, 1) # (B, H * W, 3)\n",
        "\n",
        "  return rays_o, rays_d\n",
        "\n",
        "def plucker_embedding(H, W, intrinsics, c2w, jitter=False):\n",
        "  \"\"\"Computes the plucker coordinates from batched cam2world & intrinsics matrices, as well as pixel coordinates\n",
        "  C2W : (Batch Size, 4, 4)\n",
        "  intrinsics : (Batch Size, 3, 3)\n",
        "  \"\"\"\n",
        "  rays_o, rays_d = get_rays(H, W, intrinsics, c2w, jitter=jitter) # (B, H * W, 3), (B, H * W, 3)\n",
        "  cross = torch.cross(rays_o, rays_d, dim=-1)\n",
        "  plucker = torch.cat((rays_d, cross), dim=1)\n",
        "\n",
        "  plucker = plucker.view(-1, H, W, 6).permute(0, 3, 1, 2)\n",
        "  return plucker # (B, 6, H, W, )\n",
        "\n",
        "def reference_point_plucker_embedding(H, W, intrinsics, c2w, jitter=False):\n",
        "  \"\"\"Computes the reference point plucker coordinates from batched cam2world & intrinsics matrices, as well as pixel coordinates\n",
        "  H : image height\n",
        "  W : image width\n",
        "  C2W : (Batch Size, 4, 4)\n",
        "  intrinsics : (Batch Size, 3, 3)\n",
        "  \"\"\"\n",
        "  rays_o, rays_d = get_rays(H, W, intrinsics, c2w, jitter=jitter) # (B, H * W, 3), (B, H * W, 3)\n",
        "  o_dot_d = (rays_o * rays_d).sum(dim=-1, keepdim=True) # (B, H * W , 1)\n",
        "  reference_point = rays_o - o_dot_d * rays_d # (B, H * W, 3)\n",
        "  reference_point_plucker = torch.cat((rays_d, reference_point), dim=1)\n",
        "\n",
        "  reference_point_plucker = reference_point_plucker.view(-1, H, W, 6).permute(0, 3, 1, 2)\n",
        "  return reference_point_plucker # (B, 6, H, W)"
      ],
      "metadata": {
        "id": "zyNWnQeoRcBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### 2.1.3 Loss for Training\n",
        "\n",
        "$$\\mathcal{L}_{\\text{pd}} = \\mathbb{E}_k \\left[ l_t^{(k)} - \\frac{\\mathbb{E}_k[l_t^{(k)}] - \\sigma_0 + \\mathbb{E}[o^{(k)}]}{\\sqrt{\\text{Var}(l_t^{(k)})}} \\right]$$\n",
        "\n",
        "* $\\mathcal{L}_{\\text{pd}}$ - the point distribution loss for training warm-up\n",
        "* $\\mathbb{E}$ - the mean value\n",
        "* $l_t^{(k)} = |u_t^{(k)} d^{(k)}|$\n",
        "  * $u_t^{(k)}$ - interpolated distance value between $u_{near}$ and $u_{far}$\n",
        "  * $d^{(k)}$ - the direction of the $k$-th pixel-aligned ray\n",
        "* $Var(l_t^{(k)})$ - the variance of $l_t^{(k)}$\n",
        "* $\\sigma_{0}$ - the target standard deviation (set to 0.5)\n",
        "* $o^{(k)}$ - the origin of the $k$-th pixel-aligned ray\n",
        "\n"
      ],
      "metadata": {
        "id": "QLcV_R-KQEpt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "$$\\mathcal{L}_{d e}=\\mathcal{L}_2\\left(\\hat{\\mathcal{X}}_{(0, t)}, \\mathcal{X}_0\\right)+\\lambda \\cdot \\mathcal{L}_{\\mathrm{VGG}}\\left(\\hat{\\mathcal{X}}_{(0, t)}, \\mathcal{X}_0\\right)$$\n",
        "\n",
        "* $\\hat{\\mathcal{X}}_{(0, t)}$ - the denoised multi-view images\n",
        "\n",
        "* $\\mathcal{X}_0=\\left\\{\\mathbf{x}_0^{(1)}, \\mathrm{x}_0^{(2)}, \\cdots, \\mathrm{x}_0^{(\\mathrm{N})}\\right\\}$ - N noisy views when timestep is 0\n",
        "\n",
        "* $\\mathcal{L}_2\\$ - L2 loss (=MSE loss)\n",
        "\n",
        "* $\\mathcal{L}_{\\mathrm{VGG}}\\$ - VGG Loss\n",
        "\n",
        "* $\\lambda$ - hyperparameter\n"
      ],
      "metadata": {
        "id": "brpNWBSRXFpx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/loss.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "\n",
        "def get_point_distribution_loss(rays_o, rays_d, k:int, sigma_0: float=0.5):\n",
        "  pass\n",
        "\n",
        "# Source code - https://gist.github.com/alper111/\n",
        "class VGGLoss(nn.Module):\n",
        "  def __init__(self, resize=True):\n",
        "    super().__init__()\n",
        "    blocks = []\n",
        "    VGG16 = torchvision.models.vgg16(weights=torchvision.models.VGG16_Weights.DEFAULT)\n",
        "    blocks.append(VGG16.features[:4].eval())\n",
        "    blocks.append(VGG16.features[4:9].eval())\n",
        "    blocks.append(VGG16.features[9:16].eval())\n",
        "    blocks.append(VGG16.features[16:23].eval())\n",
        "\n",
        "    for block in blocks:\n",
        "      for param in block.parameters():\n",
        "        param.requires_grad=False\n",
        "\n",
        "    self.blocks = nn.ModuleList(blocks)\n",
        "    self.transform = F.interpolate\n",
        "    self.resize = resize\n",
        "    self.register_buffer(\"mean\", torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))\n",
        "    self.register_buffer(\"std\", torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))\n",
        "\n",
        "  def forward(self, input, target, feature_layers=[0, 1, 2, 3], style_layers=[]):\n",
        "    if input.shape[1] != 3:\n",
        "      input = input.repeat(1, 3, 1, 1)\n",
        "      target = target.repeat(1, 3, 1, 1)\n",
        "    input = (input - self.mean) / self.std\n",
        "    target = (target - self.mean) / self.std\n",
        "    if self.resize:\n",
        "      input = self.transform(input, mode='bilinear', size=(224, 224), align_corners=False)\n",
        "      target = self.transform(target, mode='bilinear', size=(224, 224), align_corners=False)\n",
        "    loss = 0.\n",
        "    x = input\n",
        "    y = target\n",
        "    for i, block in enumerate(self.blocks):\n",
        "      x = block(x)\n",
        "      y = block(y)\n",
        "      if i in feature_layers:\n",
        "        loss += F.l1_loss(x, y)\n",
        "      if i in style_layers:\n",
        "        act_x = x.reshape(x.shape[0], x.shape[1], -1)\n",
        "        act_y = y.reshape(y.shape[0], y.shape[1], -1)\n",
        "        gram_x = act_x @ act_x.permute(0, 2, 1)\n",
        "        gram_y = act_y @ act_y.permute(0, 2, 1)\n",
        "        loss += F.l1_loss(gram_x, gram_y)\n",
        "    return loss\n",
        "\n",
        "def get_denoising_loss(source: torch.Tensor, target: torch.Tensor, hyperparameter: float=0.8):\n",
        "  L2_Loss = nn.MSELoss()\n",
        "  VGG_Loss = VGGLoss()\n",
        "  return L2_Loss(source, target) + hyperparameter * VGG_Loss(source, target)"
      ],
      "metadata": {
        "id": "AoGiV76QX3d2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "$$\\mathcal{L} = (\\mathcal{L}_{\\text{de}} + \\mathcal{L}_{\\text{nv}}) \\cdot \\mathbb{1}_{\\text{iter} > \\text{iter}_0} + \\mathcal{L}_{\\text{pd}} \\cdot \\mathbb{1}_{\\text{iter} \\leq \\text{iter}_0} \\cdot \\mathbb{1}_{\\text{object}}$$\n",
        "\n",
        "* $\\mathcal{L}$ - the overall training objective\n",
        "* $\\mathcal{L}_{\\text{nv}}$ - the novel view loss\n",
        "* $\\mathcal{L}_{d e}$ - the denoising loss\n",
        "* $\\mathbb{1}_{\\text{iter} > \\text{iter}_0}$ - the conditional indicator function which equals 1 if the current training iteration is greater than the threshold $iter_{0}$\n",
        "* $\\mathbb{1}_{\\text{iter} \\leq \\text{iter}_0}$  - similar indicator function as above\n"
      ],
      "metadata": {
        "id": "0eCJTxoaXFiS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from loss import get_denoising_loss\n",
        "total_iters = 100_000\n",
        "warmup_iters = 2_000\n",
        "\n",
        "current_iter = 0\n",
        "\n",
        "is_object =\n",
        "\n",
        "point_distribution_loss =\n",
        "denoising_distribution_loss = get_denoising_loss()\n",
        "novel_view_loss =\n",
        "loss = torch.where(current_iter > warmup_iters, denoising_distribution_loss + novel_view_loss, point_distribution_loss * torch.where(is_object, 1, 0))\n",
        "loss.backward()"
      ],
      "metadata": {
        "id": "MPQUInB1zBMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### 2.1.4 Training Details\n",
        "\n",
        "In page 6 of the paper,\n",
        "\n",
        "> We implement DiffusionGS by PyTorch and train it with Adam Optimizer.\n",
        "\n",
        "> To save GPU memory, we adopt mixed-precision training with BF16, sublinear memory training, and deferred GS rendering.\n",
        "\n",
        "In page 7 of the paper,\n",
        "\n",
        "> The learning rate is linearly warmed up to $4e^{-4}$ with $2K$ iterations and decades to $0$ using cosine annealing scheme.\n",
        "\n",
        "> Finally, we scale up the training resolution from $256 \\times 256$ to $512 \\times 512$ and finetune the model for $20K$ iterations.\n",
        "\n",
        "<div align=\"center\">\n",
        "  <img src=\"https://raw.githubusercontent.com/pleasure97/3D-AI-ML-Code-Implementation/main/2025/DiffusionGS/assets/Learning Rate Scheduler.png\" alt=\"Learning Rate Scheduler\">\n",
        "</div>"
      ],
      "metadata": {
        "id": "-zzpWbkwqlMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from typing import List\n",
        "\n",
        "# lrs = []\n",
        "# for i in range(total_iters):\n",
        "#     optimizer.step()\n",
        "#     scheduler.step()\n",
        "#     lrs.append(optimizer.param_groups[0]['lr'])\n",
        "\n",
        "def visualize_lr_scheduler(lrs: List[float]):\n",
        "  plt.plot(lrs)\n",
        "  plt.xlabel(\"Iteration\")\n",
        "  plt.ylabel(\"Learning Rate\")\n",
        "  plt.title(\"Linear Warm-up + Cosine Annealing\")\n",
        "  plt.show()\n",
        "\n",
        "def warmup_lambda(epoch: int, warmup_iters: int):\n",
        "  return epoch / warmup_iters if epoch < warmup_iters else 1."
      ],
      "metadata": {
        "id": "IWEcXQsQxCl3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "# Adam Optimizer\n",
        "optimizer = optim.Adam(DiffusionGS.parameters(), lr=4e-4)\n",
        "\n",
        "# Linearly Warm-Up and Cosine Annealing Scheduler\n",
        "warmup_scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=warmup_lambda)\n",
        "cosine_annealing_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_iters - warmup_iters, eta_min=0)\n",
        "scheduler = optim.lr_scheduler.SequentialLR(optimizer, schedulers=[warmup_scheduler, cosine_annealing_scheduler], milestones=[warmup_iters])"
      ],
      "metadata": {
        "id": "lwucsIyVq11x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pApealcnz2hn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 3D Diffusion\n",
        "---\n",
        "*  $\\mathbf{x}_{\\text {con }} \\in \\mathbb{R}^{H \\times W \\times 3}$ - 1 clean condition view\n",
        "* $\\mathcal{X}_t=\\left\\{\\mathbf{x}_t^{(1)}, \\mathrm{x}_t^{(2)}, \\cdots, \\mathbf{x}_t^{(N)}\\right\\}$ -  $N$ noisy views\n",
        "  * $\\mathcal{X}_0=\\left\\{\\mathbf{x}_0^{(1)}, \\mathrm{x}_0^{(2)}, \\cdots, \\mathrm{x}_0^{(\\mathrm{N})}\\right\\}$ - *Concatenated with $\\mathcal{X}_t$*\n",
        "* $\\mathbf{v}_{\\text {con }} \\in \\mathbb{R}^{H \\times W \\times 6}$ - viewpoint conditions\n",
        "  * $\\mathcal{V}=\\left\\{\\mathbf{v}^{(1)}, \\mathbf{v}^{(2)}, \\cdots, \\mathbf{v}^{(\\mathbb{N})}\\right\\}$\n",
        "\n",
        "$$\\mathbf{x}_t^{(i)}=\\overline{\\alpha_t} \\mathbf{x}_0^{(i)}+\\sqrt{1-\\overline{\\alpha_t}} \\epsilon_t^{(i)}$$\n",
        "* $\\overline{\\alpha_t}$ - pre-scheduled hyper-parameter\n",
        "* $\\epsilon_t^{(i)} \\sim \\mathcal{N}(0, \\mathbf{I})$ and $i=1,2, \\cdots, N$\n",
        "* $t$ - timestep\n"
      ],
      "metadata": {
        "id": "qYITwRYUj0AR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import tqdm\n",
        "\n",
        "HEIGHT = 256\n",
        "WIDTH = 256\n",
        "x_con = torch.randn((HEIGHT, WIDTH, 3))\n",
        "v_con = torch.randn((HEIGHT, WIDTH, 3))\n",
        "\n",
        "class DiffusionNoiser:\n",
        "  def __init__(self, noise_steps=1000, beta_start=1e-4, beta_end=2e-2, image_size=256, device=device):\n",
        "    self.noise_steps = noise_steps\n",
        "    self.beta_start = beta_start\n",
        "    self.beta_end = beta_end\n",
        "    self.image_size = image_size\n",
        "    self.device = device\n",
        "\n",
        "    self.beta = self.schedule_noise()\n",
        "    self.alpha = 1. - self.beta\n",
        "    self.alpha_hat = torch.cumprod(self.alpha, dim=0)\n",
        "\n",
        "  def schedule_noise(self):\n",
        "    return torch.linspace(self.beta_start, self.beta_end, self.noise_steps, device=self.device)\n",
        "\n",
        "  def noise_images(self, image, timestep):\n",
        "    alpha_hat_sqrt = torch.sqrt(self.alpha_hat[timestep])[:, None, None, None]\n",
        "    one_minus_alpha_hat_sqrt = torch.sqrt(1 - self.alpha_hat[timestep])[:, None, None, None]\n",
        "    epsilon = torch.rand_like(image)\n",
        "    return alpha_hat_sqrt * image + one_minus_alpha_hat_sqrt * epsilon, epsilon\n",
        "\n",
        "  def sample_timesteps(self, num_timesteps):\n",
        "    return torch.randint(low=1, high=self.noise_steps, size=(n,), device=self.device)\n",
        "\n",
        "  def sample(self, model, num_timesteps):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      x = torch.randn((num_timesteps, 3, self.image_size, self.image_size), device=device)\n",
        "      for i in tqdm(reversed(range(1, self.noise_steps)), position=0):\n",
        "        time_steps = (torch.ones(num_timesteps, device=self.device) * i).long()\n",
        "        predicted_noise = model(x, t)\n",
        "        alpha = self.alpha[time_steps][:, None, None, None]\n",
        "        alpha_hat = self.alpha_hat[time_steps][:, None, None, None]\n",
        "        beta = self.beta[time_steps][:, None, None, None]\n",
        "        if i > 1:\n",
        "          noise = torch.randn_like(x)\n",
        "        else:\n",
        "          noise = torch.zeros_like(x)\n",
        "        x = 1. / torch.sqrt(alpha) * (x - ((1 - alpha) / (torch.sqrt(1 - alpha_hat))) * predicted_noise) + torch.sqrt(beta) * noise\n",
        "    model.train()\n",
        "    x = (x.clamp(-1, 1) + 1) / 2\n",
        "    x = (x * 255).type(torch.uint8)\n",
        "    return x"
      ],
      "metadata": {
        "id": "ZA4TEh-Xzdi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Source Code link - https://huggingface.co/dylanebert/LGM-full/blob/main/pipeline.py\n",
        "# !pip install kiui\n",
        "from kiui.cam import orbit_camera\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def get_camera(num_frames, elevation=15, azimuth_start=0, azimuth_span=360, blender_coord=True, extra_view=False):\n",
        "    angle_gap = azimuth_span / num_frames\n",
        "    cameras = []\n",
        "    for azimuth in np.arange(azimuth_start, azimuth_span + azimuth_start, angle_gap):\n",
        "\n",
        "        pose = orbit_camera(-elevation, azimuth, radius=1)  # kiui's elevation is negated, [4, 4]\n",
        "\n",
        "        # opengl to blender\n",
        "        if blender_coord:\n",
        "            pose[2] *= -1\n",
        "            pose[[1, 2]] = pose[[2, 1]]\n",
        "\n",
        "        cameras.append(pose.flatten())\n",
        "\n",
        "    if extra_view:\n",
        "        cameras.append(np.zeros_like(cameras[0]))\n",
        "\n",
        "    return torch.from_numpy(np.stack(cameras, axis=0)).float()"
      ],
      "metadata": {
        "id": "oJgt8q3IMWx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Source Code link - https://huggingface.co/dylanebert/LGM-full/blob/main/pipeline.py\n",
        "# !pip install einops\n",
        "import torch\n",
        "import math\n",
        "from einops import repeat\n",
        "\n",
        "def timestep_embedding(timesteps, dim, max_period=10_000, repeat_only=False):\n",
        "    \"\"\"\n",
        "    Create sinusoidal timestep embeddings.\n",
        "    :param timesteps: a 1-D Tensor of N indices, one per batch element.\n",
        "                      These may be fractional.\n",
        "    :param dim: the dimension of the output.\n",
        "    :param max_period: controls the minimum frequency of the embeddings.\n",
        "    :return: an [N x dim] Tensor of positional embeddings.\n",
        "    \"\"\"\n",
        "    if not repeat_only:\n",
        "        half = dim // 2\n",
        "        freqs = torch.exp(-math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32, device=timesteps.device) / half)\n",
        "        args = timesteps[:, None] * freqs[None]\n",
        "        embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n",
        "        if dim % 2:\n",
        "            embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n",
        "    else:\n",
        "        embedding = repeat(timesteps, \"b -> b d\", d=dim)\n",
        "    return embedding"
      ],
      "metadata": {
        "id": "yvr1pckVH_CP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "differentiable gaussian rasterization - https://huggingface.co/learn/ml-for-3d-course/unit3/hands-on"
      ],
      "metadata": {
        "id": "4rp1gxjefm19"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "$$\\mathcal{G}_\\theta\\left(\\mathcal{X}_t \\mid \\mathbf{x}_{c o n}, \\mathbf{v}_{c o n}, t, \\mathcal{V}\\right)=\\left\\{G_t^{(k)}\\left(\\mu_t^{(k)}, \\boldsymbol{\\Sigma}_t^{(k)}, \\alpha_t^{(k)}, c_t^{(k)}\\right)\\right\\}$$\n",
        "* $\\theta$ - denoiser\n",
        "* $\\mathcal{G}_\\theta$ - predicted 3D Gaussians by $\\theta$\n",
        "* $1 \\leq k \\leq N_g$\n",
        "* $N_g=(N+1) H W$ - the number of per-pixel Gaussian $G_t^{(k)}$\n",
        "* $H , W$ - Height and Width of the image\n",
        "* $\\mu_t^{(k)} \\in$ $\\mathbb{R}^3$ - the center position of each $G_t^{(k)}$ (clipped into $[-1, 1]^3$)\n",
        "* $\\Sigma_t^{(k)} \\in \\mathbb{R}^{3 \\times 3}$ - the covariance of each $G_t^{(k)}$ controlling its shape\n",
        "  * parameterized by a rotation matrix $\\mathbf{R}_t^{(k)}$ and a scaling matrix $\\mathbf{S}_t^{(k)}$\n",
        "* $\\alpha_t^{(k)} \\in \\mathbb{R}$ - the opacity of each $G_t^{(k)}$ characterizing the transmittance\n",
        "* $c_t^{(k)} \\in \\mathbb{R}^3$ - the RGB color of each $G_t^{(k)}$\n"
      ],
      "metadata": {
        "id": "wnGtvWmg8ZaU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GaussianModel:\n",
        "  pass\n",
        "\n",
        "GaussianModel(center_positions, covariances, opacities, rgbs, time_step)"
      ],
      "metadata": {
        "id": "Gz-lYvweElDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "$$\\mu_t^{(k)}=o^{(k)}+u_t^{(k)} d^{(k)}$$\n",
        "* $o^{(k)}$ - the origin of the $k$-th pixel-aligned ray\n",
        "* $d^{(k)}$ - the direction of the $k$-th pixel-aligned ray\n"
      ],
      "metadata": {
        "id": "e8LoEsm98Zi8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "$$u_t^{(k)}=w_t^{(k)} u_{\\text {near }}+\\left(1-w_t^{(k)}\\right) u_{f a r}$$\n",
        "* $w_t^{(k)} \\in \\mathbb{R}$ - the weight to control $u_t^{(k)}$\n",
        "* $u_{\\text {near }}$ - the nearest distances\n",
        "* $u_{f a r}$ - the farthest distances\n",
        "* For the object-level Gaussian decoder, $[u_{\\text {near }}, u_{f a r}] = [0.1, 4.2]$\n",
        "* For the scene-level Gaussian decoder, $[u_{\\text {near }}, u_{f a r}] = [0, 500]$"
      ],
      "metadata": {
        "id": "yTTyvUzW8bx0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Denoiser\n",
        "\n",
        "---\n",
        "\n",
        "* $L$ - the number of transformer blocks\n",
        "* Each transformer block contains 1 MSA, 1 MLP, and 2 LN.\n",
        "* $\\hat{\\mathcal{H}}=\\left\\{\\hat{\\mathbf{H}}_{\\text {con }}, \\hat{\\mathbf{H}}^{(1)}, \\cdots, \\hat{\\mathbf{H}}^{(N)}\\right\\}$ - per-pixel Gaussian Maps\n",
        "  * $\\hat{\\mathbf{H}}_{\\text {con }}$, $\\hat{\\mathbf{H}}^{(i)} \\in$ $\\mathbb{R}^{H \\times W \\times 14}$\n",
        "\n"
      ],
      "metadata": {
        "id": "N7xysn6KpRyX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "fkh1pmEEAQzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "  \"\"\" Turns a 2D input image into a 1D sequence learnable embeding vector.\n",
        "  Args:\n",
        "    in_channels (int) - Number of color channels for the input images. Defaults to 3.\n",
        "    patch_size (int) - Size of patches to convert input images into. Defaults to 16.\n",
        "    embedding_dim (int) - Size of embedding to turn image into. Defaults to 768.\n",
        "  \"\"\"\n",
        "  def __init__(self, in_channels: int=3, patch_size: int=16, embedding_dim: int=768):\n",
        "    super().__init__()\n",
        "\n",
        "    self.in_channels = in_channels\n",
        "    self.patch_size = patch_size\n",
        "    self.embedding_dim = embedding_dim\n",
        "\n",
        "    self.patchify = nn.Conv2d(in_channels=self.in_channels,\n",
        "                              out_channels=self.embedding_dim,\n",
        "                              kernel_size=self.patch_size,\n",
        "                              stride=self.patch_size,\n",
        "                              padding=0)\n",
        "\n",
        "    self.flatten = nn.Flatten(start_dim=2, end_dim=3)\n",
        "\n",
        "  def forward(self, x: torch.Tensor):\n",
        "    image_resolution = x.shape[-1]\n",
        "    assert image_resoultion % self.patch_size == 0, \\\n",
        "      f\"Input size must be divisible by patch size, image size : {image_resolution}, patch size : {self.patch_size}\"\n",
        "\n",
        "    x_patched = self.patchify(x)\n",
        "    x_flattened = self.flatten(x_patched)\n",
        "\n",
        "    return x_flattened.permute(0, 2, 1) # [batch_size, patch_size ** 2 * channel, embedding_dim] -> [batch_size, embedding_dim, patch_size ** 2 ]\n",
        "\n"
      ],
      "metadata": {
        "id": "nWAtnG2Q9g-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo --quiet"
      ],
      "metadata": {
        "id": "Fv_vDpWZCzzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "PatchEmbedding = PatchEmbedding()\n",
        "height, width = image.shape[1], image.shape[2]\n",
        "num_patches = int((height * width) / PatchEmbedding.patch_size ** 2)\n",
        "\n",
        "# summary(PatchEmbedding(),\n",
        "#         input_size=,\n",
        "#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "#         col_width=20,\n",
        "#         row_settings=[\"var_names\"] )"
      ],
      "metadata": {
        "id": "HnRp2zbcC4CS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PositionalEmbedding = nn.Parameter(torch.ones(1, num_patches + 1, PatchEmbedding.embedding_dimension), requires_grad=True)\n",
        "PatchAndPositionalEmbedding = PatchEmbedding + PositionalEmbedding"
      ],
      "metadata": {
        "id": "XcTQolVkDGsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadSelfAttentionBlock(nn.Module):\n",
        "  def __init__(self,\n",
        "               embedding_dim: int=768,\n",
        "               num_heads: int=12,\n",
        "               attention_dropout: float=0.):\n",
        "    super().__init__()\n",
        "\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.num_heads = num_heads\n",
        "    self.attention_dropout = attention_dropout\n",
        "\n",
        "    self.layer_norm = nn.LayerNorm(normalized_shape=self.embedding_dim)\n",
        "\n",
        "    self.multihead_attention = nn.MultiheadAttention(embed_dim=self.embedding_dim,\n",
        "                                                     num_heads=self.num_heads,\n",
        "                                                     dropout=self.attention_dropout,\n",
        "                                                     batch_first=True)\n",
        "\n",
        "  def forward(self, x: torch.Tensor):\n",
        "    x = self.layer_norm(x)\n",
        "    attention_output, _ = self.multihead_attention(query=x, key=x, value=x, need_weights=False)\n",
        "    return attention_output"
      ],
      "metadata": {
        "id": "Uhu4rgEDIIz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPBlock(nn.Module):\n",
        "  def __init__(self,\n",
        "               embedding_dim: int=768,\n",
        "               mlp_size: int=3072,\n",
        "               dropout: float=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.mlp_size = mlp_size\n",
        "    self.dropout = dropout\n",
        "\n",
        "    self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n",
        "\n",
        "    self.mlp = nn.Sequential(\n",
        "        nn.Linear(in_features=self.embedding_dim, out_features=self.mlp_size),\n",
        "        nn.GELU(),\n",
        "        nn.Dropout(p=dropout),\n",
        "        nn.Linear(in_features=self.mlp_size, out_features=self.embedding_dim),\n",
        "        nn.Dropout(p=dropout)\n",
        "    )\n",
        "\n",
        "  def forward(self, x: torch.Tensor):\n",
        "    x = self.layer_norm(x)\n",
        "    x = self.mlp(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "uNz4Ae_XI9xB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self,\n",
        "               embedding_dim: int=768,\n",
        "               num_heads: int=12,\n",
        "               mlp_size: int=3072,\n",
        "               mlp_dropout: float=0.1,\n",
        "               attention_dropout: float=0.):\n",
        "    super().__init__()\n",
        "\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.num_heads = num_heads\n",
        "    self.mlp_size = mlp_size\n",
        "    self.mlp_dropout = mlp_dropout\n",
        "    self.attention_dropout = attention_dropout\n",
        "\n",
        "    self.MSABlock = MultiheadSelfAttentionBlock(embedding_dim=self.embedding_dim,\n",
        "                                                num_heads=self.num_heads,\n",
        "                                                attention_dropout=self.attention_dropout)\n",
        "\n",
        "    self.MLPBlock = MLPBlock(embedding_dim=self.embedding_dim,\n",
        "                             mlp_size=self.mlp_size,\n",
        "                             mlp_dropout=self.mlp_dropout)\n",
        "\n",
        "  def forward(self, x: torch.Tensor):\n",
        "\n",
        "    x = self.MSABlock(x) + x\n",
        "    x = self.MLPBlock(x) + x\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "v1s3sqKjNsRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerLayer(nn.Module):\n",
        "  def __init__(self,\n",
        "               img_size: int=224,\n",
        "               in_channels: int=3,\n",
        "               patch_size: int=12,\n",
        "               num_transformer_layers: int=12,\n",
        "               embedding_dim: int=768,\n",
        "               mlp_size: int=3072,\n",
        "               num_heads: int=12,\n",
        "               attention_dropout: float=0.,\n",
        "               mlp_dropout: float=0.1,\n",
        "               embedding_dropout: float=0.1,\n",
        "               num_outputs: int=3072):\n",
        "    super().__init__()\n",
        "\n",
        "    assert img_size % patch_size == 0, f\"Image size must be divisible by patch size, image size: {img_size}, patch size: {patch_size}.\"\n",
        "\n",
        "    self.img_size = img_size\n",
        "    self.in_channels = in_channels\n",
        "    self.patch_size = patch_size\n",
        "    self.num_transformer_layers = num_transformer_layers\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.mlp_size = mlp_size\n",
        "    self.num_heads = num_heads\n",
        "    self.attention_dropout = attention_dropout\n",
        "    self.mlp_dropout = mlp_dropout\n",
        "    self.embedding_dropout = embedding_dropout\n",
        "    self.num_outputs = num_outputs\n",
        "\n",
        "    self.num_patches = (self.img_size * self.img_size) // self.patch_size ** 2\n",
        "    self.PositionEmbedding = nn.Parameter(data=torch.randn(1, self.num_patches + 1, self.embedding_dim), requires_grad=True)\n",
        "    self.EmbeddingDropout = nn.Dropout(p=self.embedding_dropout)\n",
        "    self.PatchEmbedding = PatchEmbedding(in_channels=self.in_channels,\n",
        "                                         patch_size=self.patch_size,\n",
        "                                         embedding_dim=self.embedding_dim)\n",
        "    self.TransformerBlocks = nn.Sequential(*[TransformerBlock(embedding_dim=self.embedding_dim,\n",
        "                                                             num_heads=self.num_heads,\n",
        "                                                             mlp_size=self.mlp_size,\n",
        "                                                             mlp_droput=self.mlp_dropout) for _ in range(self.num_transformer_layers)])\n",
        "    self.classifier = nn.Sequential(nn.LayerNorm(normalized_shape=self.embedding_dim),\n",
        "                                    nn.Linear(in_features=self.embedding_dim, out_features=self.num_outputs))\n",
        "\n",
        "  def forward(self, x: torch.Tensor):\n",
        "    batch_size = x.shape[0]\n",
        "    x = self.PatchEmbedding(x)\n",
        "    x = self.PositionEmbedding(x) + x\n",
        "    x = self.EmbeddingDropout(x)\n",
        "    x = self.TransformerBlocks(x)\n",
        "    x = self.classifier(x)\n",
        "    return"
      ],
      "metadata": {
        "id": "6eYywT6YQ6Ds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "$$\\hat{\\mathcal{X}}_{(0, t)}=\\left\\{\\hat{\\mathbf{x}}_{(0, t)}^{(1)}, \\hat{\\mathbf{x}}_{(0, t)}^{(2)}, \\cdots, \\hat{\\mathbf{x}}_{(0, t)}^{(N)}\\right\\}$$\n",
        "* $\\hat{\\mathcal{X}}_{(0, t)}$ - the denoised multi-view images"
      ],
      "metadata": {
        "id": "BB0helHy9Co5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "$$\\hat{\\mathbf{x}}_{(0, t)}^{(i)}=F_r\\left(\\mathbf{M}_{e x t}^{(i)}, \\mathbf{M}_{i n t}^{(i)}, \\mathcal{G}_\\theta\\left(\\mathcal{X}_t \\mid \\mathbf{x}_{c o n}, \\mathbf{v}_{c o n}, t, \\mathcal{V}\\right)\\right)$$\n",
        "* $F_r$ - the differentiable rasterization function\n",
        "* $1 \\leq i \\leq N$\n",
        "* $\\mathbf{M}_{e x t}^{(i)}$ - the extrinsic matrix of the viewpoint $\\mathbf{c}^{(i)}$.\n",
        "* $\\mathbf{M}_{i n t}^{(i)}$ - the intrinsic matrix of the viewpoint $\\mathbf{c}^{(i)}$."
      ],
      "metadata": {
        "id": "RDQ7ZRcK9PhC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/graphdeco-inria/diff-gaussian-rasterization.git diff-gaussian-rasterization"
      ],
      "metadata": {
        "id": "TGtEPQCD9d8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "$$\\boldsymbol{\\Sigma}_t^{\\prime(k, i)}=\\mathbf{J}_t^{(i)} \\mathbf{W}_t^{(i)} \\boldsymbol{\\Sigma}_t^{(k)} \\mathbf{W}_t^{(i)^{\\top}} \\mathbf{J}_t^{(i)^{\\top}}$$\n",
        "* $\\boldsymbol{\\Sigma}_t^{(k)}$ - the 3D covariance matrix of each $G_t^{(k)}$ at viewpoint $\\mathbf{c}^{(i)}$ in the world coordinate system\n",
        "* $\\boldsymbol{\\Sigma}_t^{\\prime(k, i)} \\in \\mathbb{R}^{3 \\times 3}$  - the 3D covariance matrix of each $G_t^{(k)}$ at viewpoint $\\mathbf{c}^{(i)}$ in the camera coordinate system\n",
        "*  $\\mathbf{J}_t^{(i)} \\in \\mathbb{R}^{3 \\times 3}$ - the Jacobian matrix of the affine approximation of the projective transformation\n",
        "* $\\mathbf{W}_t^{(i)} \\in \\mathbb{R}^{3 \\times 3}$ - the viewing transformation"
      ],
      "metadata": {
        "id": "1WQYMNd79Pq6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "* Loading Objaverse Dataset - https://colab.research.google.com/drive/1ZLA4QufsiI_RuNlamKqV7D7mn40FbWoY\n",
        "* Paper Replicating - https://github.com/mrdbourke/pytorch-deep-learning/blob/main/08_pytorch_paper_replicating.ipynb\n",
        "* 3D Gaussian Rasterization - https://github.com/graphdeco-inria/diff-gaussian-rasterization\n",
        "* Original Plücker Coordinate - https://github.com/echen01/ray-conditioning"
      ],
      "metadata": {
        "id": "ZANdu3RzqN9-"
      }
    }
  ]
}