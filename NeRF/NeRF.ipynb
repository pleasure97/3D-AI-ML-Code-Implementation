{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a30fcf0",
   "metadata": {},
   "source": [
    "# Code Practice : NeRF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bee3a4",
   "metadata": {},
   "source": [
    "- Code Practice에서 논문의 모델을 직접 구현하고, 학습시켜, 추론하는 과정을 담으려고 합니다. \n",
    "\n",
    "\n",
    "- Code Practice에서는 코드를 짜는 과정의 시행착오를 담을 것이기 때문에 두서가 없을 수 있는 점 양해 부탁드립니다.\n",
    "\n",
    "\n",
    "- Final Code에 최종 코드가 올라갑니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b4013f",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5553b6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np \n",
    "import torch\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df42292",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540802e6",
   "metadata": {},
   "source": [
    "- 데이터셋은 tiny_nerf_data를 씁니다.\n",
    "\n",
    "\n",
    "- 데이터셋은 아래의 링크에서 다운로드 받을 수 있습니다.\n",
    "\n",
    "\n",
    "- [tiny_nerf_data](http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/ECCV20/nerf/tiny_nerf_data.npz)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ca851769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The array name of tiny_nerf_data:  ['images', 'poses', 'focal']\n",
      "images : (106, 100, 100, 3)\n",
      "poses : (106, 4, 4)\n",
      "focal : ()\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14808\\3851653895.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;31m# Move training variables to the device\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[0mimages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m...\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[0mposes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mposes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[0mfocal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfocal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \"multiprocessing, you must use the 'spawn' start method\")\n\u001b[0;32m    220\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_cuda_getDeviceCount'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 221\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_cudart\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m             raise AssertionError(\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "# Specify the environment path\n",
    "PATH = 'C:/Users/user/anaconda3/envs/NeRF' # check your development environment path\n",
    "\n",
    "# Load tiny_nerf_data\n",
    "tiny_nerf_data = np.load(os.path.join(PATH, 'tiny_nerf_data.npz'))\n",
    "\n",
    "# Check the array name of tiny_nerf_data\n",
    "print('The array name of tiny_nerf_data: ', tiny_nerf_data.files)\n",
    "\n",
    "# Check the array shape of tiny_nerf_data\n",
    "for name in tiny_nerf_data.files:\n",
    "    print(name,':', tiny_nerf_data[name].shape)\n",
    "\n",
    "# Set the device\n",
    "device = torch.device('cuda'if torch.cuda.is_available else 'cpu')\n",
    "\n",
    "# Define variables of dataset\n",
    "images = tiny_nerf_data['images']\n",
    "poses = tiny_nerf_data['poses']\n",
    "focal = tiny_nerf_data['focal']\n",
    "\n",
    "# Get the number of images, and the length of height and width\n",
    "num_images, height, width = images.shape[:-1]\n",
    "\n",
    "# Split the dataset into training set and test set \n",
    "test_idx = 101\n",
    "test_image = images[test_idx]\n",
    "test_pose = poses[test_idx]\n",
    "\n",
    "# Move training variables to the device\n",
    "images = torch.from_numpy(images[:100, ... , :3]).to(device)\n",
    "poses = torch.from_numpy(poses).to(device)\n",
    "focal = torch.from_numpy(focal).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "141c31b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f31b799",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0fb08abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding\n",
    "def positional_encoding(p : torch.tensor, L : int) -> torch.tensor:\n",
    "    '''\n",
    "    Inputs:\n",
    "        p - Tensor. p can be 3 coordinate values in vector x or Catersian viewing direction unit vector d.\n",
    "            p lies in [-1, 1].\n",
    "        L - Int. The number of dimensions.\n",
    "    \n",
    "    Output:\n",
    "        gamma_p - Tensor. The positional encoding of p.\n",
    "    '''\n",
    "    gamma_p =[]\n",
    "    \n",
    "    frequency = 2.0 ** torch.linspace(0, L-1, L, dtype = p.dtype, device = p.device)\n",
    "    \n",
    "    for freq in frequency:\n",
    "        gamma_p.append(freq * torch.pi * p)\n",
    "        gamma_p.append(freq * torch.pi * p)\n",
    "        \n",
    "    gamma_p = torch.cat(gamma_p, dim = -1)\n",
    "    \n",
    "    return gamma_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9ff8226c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -1.6022,   0.3927,   2.1363,  -1.6022,   0.3927,   2.1363,  -3.2044,\n",
      "          0.7854,   4.2726,  -3.2044,   0.7854,   4.2726,  -6.4088,   1.5708,\n",
      "          8.5451,  -6.4088,   1.5708,   8.5451, -12.8177,   3.1416,  17.0903,\n",
      "        -12.8177,   3.1416,  17.0903])\n"
     ]
    }
   ],
   "source": [
    "p = torch.Tensor([-0.51, 0.125, 0.68])\n",
    "print(positional_encoding(p, L = 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cd1c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classical Volume Rendering\n",
    "def classical_volume_rendering(t_n, t_f, N):\n",
    "    '''\n",
    "    Inputs:\n",
    "        t_n - the nearest boundary point of a camera ray. \n",
    "        t_f - the farthest boundary point of a camera ray. \n",
    "        N - \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60933d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified Sampling\n",
    "def stratified_sampling()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
