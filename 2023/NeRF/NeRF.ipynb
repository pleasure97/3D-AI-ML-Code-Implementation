{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a30fcf0",
   "metadata": {},
   "source": [
    "# Code Practice : NeRF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b4013f",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "694929fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df42292",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540802e6",
   "metadata": {},
   "source": [
    "- 데이터셋은 tiny_nerf_data를 씁니다.\n",
    "\n",
    "\n",
    "- 데이터셋은 아래의 링크에서 다운로드 받을 수 있습니다.\n",
    "\n",
    "\n",
    "- [tiny_nerf_data](http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/ECCV20/nerf/tiny_nerf_data.npz)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca851769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The array name of tiny_nerf_data:  ['images', 'poses', 'focal']\n",
      "images : (106, 100, 100, 3)\n",
      "poses : (106, 4, 4)\n",
      "focal : ()\n"
     ]
    }
   ],
   "source": [
    "# Specify the environment path\n",
    "PATH = 'C:/Users/user/anaconda3/envs/NeRF' # check your development environment path\n",
    "\n",
    "# Load tiny_nerf_data\n",
    "tiny_nerf_data = np.load(os.path.join(PATH, 'tiny_nerf_data.npz'))\n",
    "\n",
    "# Check the array name of tiny_nerf_data\n",
    "print('The array name of tiny_nerf_data: ', tiny_nerf_data.files)\n",
    "\n",
    "# Check the array shape of tiny_nerf_data\n",
    "for name in tiny_nerf_data.files:\n",
    "    print(name,':', tiny_nerf_data[name].shape)\n",
    "\n",
    "# Set the device\n",
    "device = torch.device('cuda'if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define variables of dataset\n",
    "images = tiny_nerf_data['images']\n",
    "poses = tiny_nerf_data['poses']\n",
    "focal = tiny_nerf_data['focal']\n",
    "\n",
    "# Get the number of images, and the length of height and width\n",
    "num_images, height, width = images.shape[:-1]\n",
    "\n",
    "# Split the dataset into training set and test set \n",
    "test_idx = 101\n",
    "test_image = images[test_idx]\n",
    "test_pose = poses[test_idx]\n",
    "\n",
    "# Move training variables to the device\n",
    "images = torch.from_numpy(images[:100, ... , :3]).to(device)\n",
    "poses = torch.from_numpy(poses).to(device)\n",
    "focal = torch.from_numpy(focal).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97408785",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e85d7930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the origin and direction vectors \n",
    "def get_rays(height, width, focal, pose) : \n",
    "    '''\n",
    "    Inputs:\n",
    "        height - Int. the height of an image\n",
    "        width - Int. the width of an image\n",
    "        focal - Float. focal length of the camera\n",
    "        pose - torch.Tensor. the pose of an image \n",
    "        \n",
    "    Outputs:\n",
    "        rays_o - torch.Tensor. origin vector of the ray\n",
    "        rays_d - torch.Tensor. direction vector of the ray\n",
    "    '''\n",
    "    \n",
    "    # Use torch.meshgrid to build a meshgrid of size (height X width)\n",
    "    i, j = torch.meshgrid(torch.arange(width, dtype = torch.float32).to(pose), \n",
    "                          torch.arange(height, dtype = torch.float32).to(pose), \n",
    "                          indexing = 'ij')\n",
    "    \n",
    "    # Use .transpose method to reshape the meshgrid\n",
    "    i, j = i.transpose(-1, -2), j.transpose(-1, -2)\n",
    "    \n",
    "    # Calculate the x,y,z coordinates of a direction vector, ray_d\n",
    "    # Measure the distance between current pixel coordinates and the center of the image on x,y axis.\n",
    "    # Normalize x,y coordinates and set a z coordinate to 1. \n",
    "    rays_d = torch.stack([i - weight * .5 / focal, -(j - height * .5) / focal, torch.ones_like(i)], dim = -1)\n",
    "    \n",
    "    # Multiply rays_d with the camera pose to rotate w.r.t. world coordinates\n",
    "    rays_d = torch.sum(rays_d[..., None, :] * pose[:3, :3], dim = -1) # pose[:3, :3] is the rotation part.\n",
    "    \n",
    "    rays_o = pose[:3, -1].expand(rays_d.shape)\n",
    "    \n",
    "    return rays_o, rays_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45636136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding\n",
    "def positional_encoding(p, L):\n",
    "    '''\n",
    "    Inputs:\n",
    "        p - torch.Tensor. p can be 3 coordinate values in vector x or Catersian viewing direction unit vector d.\n",
    "            p lies in [-1, 1].\n",
    "        L - Int. Dimensionality of positional encoding.\n",
    "    \n",
    "    Output:\n",
    "        gamma_p - torch.Tensor. The positional encoding of p.\n",
    "    '''\n",
    "    # Define the list that saves positional encoding parameters\n",
    "    gamma_p = []\n",
    "    \n",
    "    # Define the frequency that maps to higher dimensional space\n",
    "    frequency = 2.0 ** torch.linspace(0, L-1, L, dtype = p.dtype, device = p.device)\n",
    "    \n",
    "    # Iterate for the number of frequency to append related positional encoding\n",
    "    for freq in frequency:\n",
    "        gamma_p.append(freq * torch.pi * p)\n",
    "        gamma_p.append(freq * torch.pi * p)\n",
    "        \n",
    "    gamma_p = torch.concat(gamma_p, dim = -1)\n",
    "    \n",
    "    return gamma_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c711053d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -1.6022,   0.3927,   2.1363,  -1.6022,   0.3927,   2.1363,  -3.2044,\n",
      "          0.7854,   4.2726,  -3.2044,   0.7854,   4.2726,  -6.4088,   1.5708,\n",
      "          8.5451,  -6.4088,   1.5708,   8.5451, -12.8177,   3.1416,  17.0903,\n",
      "        -12.8177,   3.1416,  17.0903])\n"
     ]
    }
   ],
   "source": [
    "p = torch.Tensor([-0.51, 0.125, 0.68])\n",
    "print(positional_encoding(p, L = 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd8e90b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified Sampling\n",
    "def stratified_sampling(rays_o, rays_d, t_n, t_f, N):\n",
    "    '''\n",
    "    Inputs :\n",
    "        rays_o - torch.Tensor. Shape of (width, height, 3). origin vector of the camera ray\n",
    "        rays_d - torch.Tensor. Shape of (width, height, 3). direction vector of the camera ray\n",
    "        t_n - Float. the nearest boundary point of the camera ray\n",
    "        t_f - Float. the farthest boundary point of the camera ray\n",
    "        N - Int. the number of bins to partition [t_n, t_f]\n",
    "        \n",
    "    Outputs : \n",
    "        t - torch.Tensor. Shape of (N,). linspace divided into N intervals. \n",
    "        x - torch.Tensor. Shape of (width, height, N, 3). the vector that satisfies 'x = o + td'\n",
    "    '''\n",
    "    \n",
    "    # Partition the interval [t_n, t_f] by N and move the variable to the device.\n",
    "    t = torch.linspace(t_n, t_f, N).to(rays_o)\n",
    "    \n",
    "    # Generate random noise that matches a given shape (width, height, N) and a given boundary [t_n, t_f]\n",
    "    noise = torch.rand(rays_o.shape[:-1] + (N,)) * (t_f - t_n) / N \n",
    "    \n",
    "    # Add the noise to t.\n",
    "    t += noise\n",
    "    \n",
    "    # Calculate the equation of r and match the shape of (width, height, N, 3).\n",
    "    x = rays_o[..., None, :] + t[..., None] * rays_d[..., None, :]\n",
    "    \n",
    "    return x, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b65cffb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classical Volume Rendering\n",
    "def classical_volume_rendering(rays_o, t, sigma, c):\n",
    "    '''\n",
    "    Inputs:\n",
    "        rays_o - torch.Tensor. Shape of (width, height, 3). origin vector of the camera ray\n",
    "        t - torch.Tensor. Shape of (N,). linspace divided into N intervals\n",
    "        sigma - torch.Tensor. Tensor of volume density\n",
    "        c - torch.Tensor. Tensor of \n",
    "        \n",
    "    Outputs : \n",
    "        rgb - torch.Tensor. rendered color by classical volume rendering\n",
    "    \n",
    "    '''\n",
    "    # Use ReLU function to ensure that the volume density can be non-negative.\n",
    "    sigma = F.relu(sigma)[..., 0]\n",
    "    \n",
    "    # Use sigmoid function to ensure that the color vector, c can be in [0, 1].\n",
    "    c = torch.sigmoid(c)\n",
    "    \n",
    "    # Calculate the distance between adjacent samples, delta.\n",
    "    delta = t[1:, ...] - t[:-1, ...]\n",
    "    \n",
    "    # Add the endpoint of the ray to delta and assume that the endpoint of the ray has a very large value.\n",
    "    delta = torch.cat([delta, torch.Tensor([1e10], dtype = rays_o.dtype, device = rays_o.device)], dim = -1)\n",
    "    \n",
    "    # Calculate the alpha compositing value, alpha. \n",
    "    alpha = 1. - torch.exp(-sigma * delta)\n",
    "    \n",
    "    # Product (1 - alpha) cumulatively and prevent multiplication by 0. \n",
    "    T = torch.cumprod(1. - alpha + 1e-10, -1)\n",
    "    \n",
    "    # Use torch.roll to remove the 'i'th multiplied case.\n",
    "    T = torch.roll(T, 1, -1)\n",
    "    \n",
    "    # Fill the first case of T with 1. \n",
    "    T[..., 0] = 1.\n",
    "    \n",
    "    # Multiply T with alpha before aligining the shape with c. \n",
    "    w = T * alpha \n",
    "    \n",
    "    rgb = (w[..., None] * c).sum(dim = -2)\n",
    "    \n",
    "    return rgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26837983",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c04a413",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeRF(nn.Module):\n",
    "    def __init__(self, dim_x = 60, dim_d = 24, num_layers = 8, num_channels = 256, skip_connection = 4):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = []\n",
    "        \n",
    "        for i in range(num_layers):\n",
    "            if i == 0:\n",
    "                layers.append(nn.Linear(dim_x, num_channels))\n",
    "            elif i == skip_connection : \n",
    "                layers.append(nn.Linear(num_channels + dim_x, num_channels))\n",
    "            else :\n",
    "                layers.append(nn.Linear(num_channels, num_channels))\n",
    "        \n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.fc1 = nn.Linear(num_channels, num_channels)\n",
    "        self.fc2 = nn.Linear(num_channels + dim_d, num_channels // 2)\n",
    "        \n",
    "        self.sigma = nn.Linear(num_channels, 1)\n",
    "        self.rgb = nn.Linear(num_channels // 2, 3)\n",
    "        \n",
    "        self.skip_connection = skip_connection\n",
    "                \n",
    "    \n",
    "    def forward(self, x, d):\n",
    "        out = x \n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if i == skip_connection:\n",
    "                out = torch.cat([out, x], dim = -1)\n",
    "                out = self.layers[i](out)\n",
    "                out = F.relu(out)\n",
    "            else :\n",
    "                out = self.layers[i](out)\n",
    "                out = F.relu(out)\n",
    "        \n",
    "        sigma = self.sigma(out)\n",
    "        \n",
    "        out = self.fc1(out)\n",
    "        out = torch.concat([out, d], dim = -1)\n",
    "        out = self.fc2(out)\n",
    "        out = F.relu(out)\n",
    "        rgb = self.rgb(out)\n",
    "        \n",
    "        return rgb, sigma\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e697e59",
   "metadata": {},
   "source": [
    "## Defining Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93b3b49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters of positional encoding\n",
    "L_X = 10\n",
    "L_D = 4 \n",
    "\n",
    "# hyperparameters of stratified sampling\n",
    "T_N = 2.\n",
    "T_F = 6.\n",
    "N = 32\n",
    "\n",
    "# hyperparameters of NeRF model \n",
    "NUM_LAYERS = 8 \n",
    "NUM_CHANNELS = 256\n",
    "SKIP_CONNECTION = 4 \n",
    "\n",
    "# hyperparameters of training\n",
    "EPOCHS = 10000\n",
    "BATCH_SIZE = 4096\n",
    "\n",
    "# Learning rate of the Adam optimizer\n",
    "LR = 5e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f40e6f",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f363a563",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, height, width, focal, pose, T_N, T_F, N, L_X, L_D):\n",
    "    '''\n",
    "    Inputs :\n",
    "        height - Int. the height of an image\n",
    "        width - Int. the width of an image\n",
    "        focal - Float. focal length of the camera\n",
    "        pose - torch.Tensor. the pose of an image \n",
    "        T_N - Float. the nearest boundary point of the camera ray\n",
    "        T_F - Float. the farthest boundary point of the camera ray\n",
    "        N - Int. the number of bins to partition [T_N, T_F] \n",
    "        L_X - Int. the dimensionality of positional encoding for x\n",
    "        L_D - Int. the dimensionality of positional encoding for d  \n",
    "    \n",
    "    Output :\n",
    "        rgb - torch.Tensor. rendered color by classical volume rendering\n",
    "    '''\n",
    "    rays_o, rays_d = get_rays(height, width, focal, pose)\n",
    "    \n",
    "    x, t = stratified_sampling(rays_o, rays_d, t_n, t_f, N)\n",
    "    \n",
    "    # Flatten the ray vector and directional vector to have a shape of (width * height * N , 3)\n",
    "    x_flatten = x.reshape(-1, 3) # x has a shape of (width, height, N, 3)\n",
    "    d_flatten = rays_d[..., None, :].expand_as(x).reshape(-1, 3) # d has a shape of (width, height, 3)\n",
    "    \n",
    "    gamma_x = positional_encoding(x_flatten, L_X)\n",
    "    gamma_d = positional_encoding(d_flatten, L_D)\n",
    "    \n",
    "    # Set the list to save predictions\n",
    "    preds = []\n",
    "    \n",
    "    # Slice the training data by batch size, predict the model's outputs, and append them to the list\n",
    "    for i in range(0, gamma_x.shape[0], batch_size):\n",
    "        preds.append(model(gamma_x[i : i + batch_size], gamma_d[i : i + batch_size]))\n",
    "    \n",
    "    # Define color and sigma variables from predictions\n",
    "    color = torch.concat([pred[0] for pred in preds], dim = 0).reshape(height, width, -1, 3)\n",
    "    sigma = torch.concat([pred[1] for pred in preds], dim = 0).reshape(height, width, -1, 1)\n",
    "    \n",
    "    rgb = classical_volume_rendering(color, sigma, rays_o, t)\n",
    "    \n",
    "    return rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae8b88b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18788\\734470273.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m model = NeRF(dim_x = gamma_x_dim, dim_d = gamma_d_dim, \\\n\u001b[0;32m      5\u001b[0m              num_layers = NUM_LAYERS, num_channels = NUM_CHANNELS, skip_connection = SKIP_CONNECTION)\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "gamma_x_dim = 3 * 2 * L_X\n",
    "gamma_d_dim = 3 * 2 * L_D\n",
    "\n",
    "model = NeRF(dim_x = gamma_x_dim, dim_d = gamma_d_dim, \\\n",
    "             num_layers = NUM_LAYERS, num_channels = NUM_CHANNELS, skip_connection = SKIP_CONNECTION)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LR)\n",
    "\n",
    "seed = 9458\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "pbar = tqdm(range(num_iters))\n",
    "for i in pbar:\n",
    "    idx = np.random.randint(images.shape[0])\n",
    "    image_i = images[idx]\n",
    "    pose_i = poses[idx]\n",
    "    \n",
    "    rgb_predicted = train(model, height, width, focal, pose, T_N, T_F, N, L_X, L_D)\n",
    "    loss = F.mse_loss(rgb_predicted, image_i)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
